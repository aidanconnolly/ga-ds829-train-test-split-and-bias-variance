{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train-Test-Split and Bias and Variance\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kevin Markham(DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Describe what error due to bias is and what error due to variance is\n",
    "- Identify the bias-variance tradeoff\n",
    "- Describe what overfitting and underfitting means in the context of model building\n",
    "- Explain problems associated with over and underfitting\n",
    "- Grasp why train, test split is necessary\n",
    "- Explore kfolds, LOOCV, and three split methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Bias and Variance Trade-off](#bias-and-variance-trade-off)\n",
    "\t- [Bias? Variance?](#bias-variance)\n",
    "\t- [Exploring the Bias-Variance Tradeoff](#exploring-the-bias-variance-tradeoff)\n",
    "\t- [Brain and body weight mammal dataset](#brain-and-body-weight-mammal-dataset)\n",
    "\t- [Making a prediction](#making-a-prediction)\n",
    "- [Making a prediction from a sample](#making-a-prediction-from-a-sample)\n",
    "\t- [Let's try something completely different](#lets-try-something-completely-different)\n",
    "- [Balancing Bias and Variance](#balancing-bias-and-variance)\n",
    "- [Train-test-split](#train-test-split)\n",
    "\t- [Evaluation procedure #1: Train and test on the entire dataset (do not do this)](#evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this)\n",
    "\t- [Problems with training and testing on the same data](#problems-with-training-and-testing-on-the-same-data)\n",
    "\t- [Evaluation procedure #2: Train/test split](#evaluation-procedure--traintest-split)\n",
    "\t- [Comparing test performance with a null baseline](#comparing-test-performance-with-a-null-baseline)\n",
    "- [K-folds cross-validation](#k-folds-cross-validation)\n",
    "\t- [Leave-one-out-cross-validation](#leave-one-out-cross-validation)\n",
    "\t- [Intro to cross validation with the Boston data](#intro-to-cross-validation-with-the-boston-data)\n",
    "- [Three way data split](#three-way-data-split)\n",
    "\t- [Additional Resources](#additional-resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-and-variance-trade-off\"></a>\n",
    "## Bias and Variance Trade-off\n",
    "---\n",
    "\n",
    "**Bias** is error due to the difference between the correct model and our predicted value.\n",
    "\n",
    "**Variance** is the error due to the variability of a model for a given data point.\n",
    "\n",
    "As model complexity **increases**, bias **decreases**.\n",
    "\n",
    "As model complexity **increases**, variance **increases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance\"></a>\n",
    "### Bias? Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Conceptual Definitions**\n",
    "- Bias – Error that results from the correct value and\n",
    "the predicted value within our model\n",
    "  - Roughly, whether our model aims on target or not.\n",
    "- Variance – Error due to the variability of a model\n",
    "prediction for a given data point\n",
    "  - Roughly, whether our model is reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/biasVsVarianceImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Visually, we are building a\n",
    "model where the bulls-eye is\n",
    "the goal\n",
    "- Each individual hit is one\n",
    "prediction based on our model\n",
    "- Critically, the success of our\n",
    "model (low variance, low bias)\n",
    "depends on the training data\n",
    "present "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Mathematically, we are explaining a linear relationship dependent on some function**\n",
    "- The error of our prediction is equal to the\n",
    "true outcome and our predicted outcome\n",
    "- This can be decomposed into two parts: the\n",
    "bias and the variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "### Exploring the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"brain-and-body-weight-mammal-dataset\"></a>\n",
    "### Brain and body weight mammal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a [dataset](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body and the brain for 62 mammal species. Let's read it into pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../assets/dataset/mammals.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-348e516041ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'../assets/dataset/mammals.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'brain'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmammals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmammals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File ../assets/dataset/mammals.txt does not exist"
     ]
    }
   ],
   "source": [
    "path = r'../assets/dataset/mammals.txt'\n",
    "cols = ['brain','body']\n",
    "mammals = pd.read_table(path, sep='\\t', names=cols, header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mammals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-14f75e873734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmammals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mammals' is not defined"
     ]
    }
   ],
   "source": [
    "mammals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus on a smaller subset in which the body weight is less than 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep rows in which the body weight is less than 200\n",
    "mammals = mammals[mammals.body < 200]\n",
    "mammals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're now going to pretend that there are only 51 mammal species in existence. In other words, we are pretending that this is the entire dataset of brain and body weights for **every known mammal species**.\n",
    "\n",
    "Let's create a scatterplot (using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)) to visualize the relationship between brain and body weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, fit_reg=False);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"making-a-prediction\"></a>\n",
    "### Making a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-quick-review\"></a>\n",
    "#### Linear regression quick review\n",
    "\n",
    "![](./assets/images/linear-residuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's pretend that a **new mammal species** is discovered. We measure the body weight of every member of this species that we can find, and calculate an **average body weight of 100**. We want to **predict the average brain weight** of this species (rather than measuring it directly). How might we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We drew a straight line that appears to best capture the relationship between brain and body weight. So, we might predict that our new species has a brain weight of about 45, since that's the approximate y value when x=100.\n",
    "\n",
    "This is known as a \"linear model\" or a \"linear regression model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"making-a-prediction-from-a-sample\"></a>\n",
    "## Making a prediction from a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Earlier, I said that this dataset contained every known mammal species. That's very convenient, but **in the real world, all you ever have is a sample of data**. A more realistic situation would be to only have brain and body weights for (let's say) half of the 51 known mammals.\n",
    "\n",
    "When that new mammal species (with a body weight of 100) is discovered, we still want to make an accurate prediction for the brain weight, but this task might be more difficult since we don't have all of the data that we would ideally like to have.\n",
    "\n",
    "Let's simulate this situation by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# randomly assign every observation to either universe 1 or universe 2\n",
    "mammals['universe'] = np.random.randint(1, 3, len(mammals))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Important:** We only live in one of the two universes. Both universes have 51 known mammal species, but each universe knows the brain and body weight for different species.\n",
    "\n",
    "We can now tell Seaborn to create two plots, in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe');\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data. In both cases, we would predict a brain weight of about 45.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe');\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What was the point of this exercise? This was a visual demonstration of a high bias, low variance model:\n",
    "\n",
    "- It's **high bias** because it doesn't fit the data particularly well.\n",
    "- It's **low variance** because it doesn't change much depending on which observations happen to be available in that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "### Let's try something completely different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What would a **low bias, high variance** model look like? Let's try polynomial regression, with an eighth order polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=8);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It's **low bias** because the models match the data quite well!\n",
    "- It's **high variance** because the models are widely different depending on which observations happen to be available in that universe. (For a body weight of 100, the brain weight prediction would be 40 in one universe and 0 in the other universe!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe', order=8);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## Balancing Bias and Variance\n",
    "Can we find a middle ground?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perhaps we can create a model that has **less bias than the linear model**, and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Let's try a second order polynomial instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=2);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This seems better. In both the left and right plots, **it fits the data pretty well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance tradeoff**: You are seeking a model that appropriately balances bias and variance, and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want a model that best balances bias and variance. It\n",
    "should match our training data well (moderate bias) yet be low variance for out-of-sample data (moderate variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Training error as a function of\n",
    "complexity.\n",
    "- Question: why do we even\n",
    "care about variance if we\n",
    "know we can generate a\n",
    "more accurate model with\n",
    "higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-test-split\"></a>\n",
    "## Train-test-split\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into a problem where powerful models can perfectly fit the data that they are trained on. These models are **low bias** and **high variance**. However, we can't observe the variance of a model directly, because we only know how it fits on the data we have and not all potential samples.\n",
    "\n",
    "**Solution:** Create a procedure that **estimates** how well a model is likely to perform on out-of-sample data and use that to choose between models.\n",
    "\n",
    "**Note:** These procedures can be used with **any machine learning model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The holdout method: Train/Test Split**\n",
    "- Training Set: Used to train the classifier\n",
    "- Testing Set: Used to estimate the error rate of the trained classifier\n",
    "- Advantages? Fast! Simple! Computationally inexpensive!\n",
    "- Disadvantages? Eliminating data! Imperfect splits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this\"></a>\n",
    "### Evaluation procedure #1: Train and test on the entire dataset (do not do this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Train the model on the **entire dataset**.\n",
    "2. Test the model on the **same dataset**, and evaluate how well we did by comparing the **predicted** response values with the **true** response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an X and y variable to store the feature matrix and response from the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for both parts of data, dont' forget to assign column names\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate y and X, then overwrite the Boston variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = pd.concat([y, X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform basic EDA to make sure the data is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV       0\n",
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV       float64\n",
       "CRIM       float64\n",
       "ZN         float64\n",
       "INDUS      float64\n",
       "CHAS       float64\n",
       "NOX        float64\n",
       "RM         float64\n",
       "AGE        float64\n",
       "DIS        float64\n",
       "RAD        float64\n",
       "TAX        float64\n",
       "PTRATIO    float64\n",
       "B          float64\n",
       "LSTAT      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.532806</td>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.197104</td>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.025000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.200000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MEDV        CRIM          ZN       INDUS        CHAS         NOX  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    22.532806    3.593761   11.363636   11.136779    0.069170    0.554695   \n",
       "std      9.197104    8.596783   23.322453    6.860353    0.253994    0.115878   \n",
       "min      5.000000    0.006320    0.000000    0.460000    0.000000    0.385000   \n",
       "25%     17.025000    0.082045    0.000000    5.190000    0.000000    0.449000   \n",
       "50%     21.200000    0.256510    0.000000    9.690000    0.000000    0.538000   \n",
       "75%     25.000000    3.647423   12.500000   18.100000    0.000000    0.624000   \n",
       "max     50.000000   88.976200  100.000000   27.740000    1.000000    0.871000   \n",
       "\n",
       "               RM         AGE         DIS         RAD         TAX     PTRATIO  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     6.284634   68.574901    3.795043    9.549407  408.237154   18.455534   \n",
       "std      0.702617   28.148861    2.105710    8.707259  168.537116    2.164946   \n",
       "min      3.561000    2.900000    1.129600    1.000000  187.000000   12.600000   \n",
       "25%      5.885500   45.025000    2.100175    4.000000  279.000000   17.400000   \n",
       "50%      6.208500   77.500000    3.207450    5.000000  330.000000   19.050000   \n",
       "75%      6.623500   94.075000    5.188425   24.000000  666.000000   20.200000   \n",
       "max      8.780000  100.000000   12.126500   24.000000  711.000000   22.000000   \n",
       "\n",
       "                B       LSTAT  \n",
       "count  506.000000  506.000000  \n",
       "mean   356.674032   12.653063  \n",
       "std     91.294864    7.141062  \n",
       "min      0.320000    1.730000  \n",
       "25%    375.377500    6.950000  \n",
       "50%    391.440000   11.360000  \n",
       "75%    396.225000   16.955000  \n",
       "max    396.900000   37.970000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a feature matrix, X, and reponse, y,  for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature matrix (X)\n",
    "feature_cols = boston.columns.drop(['MEDV'])\n",
    "X = boston[feature_cols]\n",
    "# create response vector (y)\n",
    "y = boston.MEDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import linear regression, instantiate, fit, and  preview predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.00821269,  25.0298606 ,  30.5702317 ,  28.60814055,\n",
       "        27.94288232,  25.25940048,  23.00433994,  19.5347558 ,\n",
       "        11.51696539,  18.91981483,  18.9958266 ,  21.58970854,\n",
       "        20.90534851,  19.55535931,  19.2837957 ,  19.30000174,\n",
       "        20.52889993,  16.9096749 ,  16.17067411,  18.40781636,\n",
       "        12.52040454,  17.67104565,  15.82934891,  13.80368317,\n",
       "        15.67708138,  13.3791645 ,  15.46258829,  14.69863607,\n",
       "        19.54518512,  20.87309945,  11.44806825,  18.05900412,\n",
       "         8.78841666,  14.27882319,  13.69097132,  23.81755469,\n",
       "        22.34216285,  23.11123204,  22.91494157,  31.35826216,\n",
       "        34.21485385,  28.0207132 ,  25.20646572,  24.61192851,\n",
       "        22.94438953,  22.10150945,  20.42467417,  18.03614022,\n",
       "         9.10176198,  17.20856571,  21.28259372,  23.97621248,\n",
       "        27.65853521,  24.0521088 ,  15.35989132,  31.14817003,\n",
       "        24.85878746,  33.11017111,  21.77458036,  21.08526739,\n",
       "        17.87203538,  18.50881381,  23.9879809 ,  22.54944098,\n",
       "        23.37068403,  30.36557584,  25.53407332,  21.11758504,\n",
       "        17.42468223,  20.7893086 ,  25.20349174,  21.74490595,\n",
       "        24.56275612,  24.04479519,  25.5091157 ,  23.97076758,\n",
       "        22.94823519,  23.36106095,  21.26432549,  22.4345376 ,\n",
       "        28.40699937,  26.99734716,  26.03807246,  25.06152125,\n",
       "        24.7858613 ,  27.79291889,  22.16927073,  25.89685664,\n",
       "        30.67771522,  30.83225886,  27.12127354,  27.41597825,\n",
       "        28.9456478 ,  29.08668003,  27.04501726,  28.62506705,\n",
       "        24.73038218,  35.78062378,  35.11269515,  32.25115468,\n",
       "        24.57946786,  25.59386215,  19.76439137,  20.31157117,\n",
       "        21.4353635 ,  18.53971968,  17.18572611,  20.74934949,\n",
       "        22.64791346,  19.77000977,  20.64745349,  26.52652691,\n",
       "        20.77440554,  20.71546432,  25.17461484,  20.4273652 ,\n",
       "        23.37862521,  23.69454145,  20.33202239,  20.79378139,\n",
       "        21.92024414,  22.47432006,  20.55884635,  16.36300764,\n",
       "        20.56342111,  22.48570454,  14.61264839,  15.1802607 ,\n",
       "        18.93828443,  14.0574955 ,  20.03651959,  19.41306288,\n",
       "        20.06401034,  15.76005772,  13.24771577,  17.26167729,\n",
       "        15.87759672,  19.36145104,  13.81270814,  16.44782934,\n",
       "        13.56511101,   3.98343974,  14.59241207,  12.14503093,\n",
       "         8.72407108,  12.00815659,  15.80308586,   8.50963929,\n",
       "         9.70965512,  14.79848067,  20.83598096,  18.30017013,\n",
       "        20.12575267,  17.27585681,  22.35997992,  20.07985184,\n",
       "        13.59903744,  33.26635221,  29.03938379,  25.56694529,\n",
       "        32.71732164,  36.78111388,  40.56615533,  41.85122271,\n",
       "        24.79875684,  25.3771545 ,  37.20662185,  23.08244608,\n",
       "        26.40326834,  26.65647433,  22.55412919,  24.2970948 ,\n",
       "        22.98024802,  29.07488389,  26.52620066,  30.72351225,\n",
       "        25.61835359,  29.14203283,  31.43690634,  32.9232938 ,\n",
       "        34.72096487,  27.76792733,  33.88992899,  30.99725805,\n",
       "        22.72124288,  24.76567683,  35.88131719,  33.42696242,\n",
       "        32.41513625,  34.51611818,  30.76057666,  30.29169893,\n",
       "        32.92040221,  32.11459912,  31.56133385,  40.84274603,\n",
       "        36.13046343,  32.66639271,  34.70558647,  30.09276228,\n",
       "        30.64139724,  29.29189704,  37.07062623,  42.02879611,\n",
       "        43.18582722,  22.6923888 ,  23.68420569,  17.85435295,\n",
       "        23.49543857,  17.00872418,  22.39535066,  17.06152243,\n",
       "        22.74106824,  25.21974252,  11.10601161,  24.51300617,\n",
       "        26.60749026,  28.35802444,  24.91860458,  29.69254951,\n",
       "        33.18492755,  23.77145523,  32.14086508,  29.74802362,\n",
       "        38.36605632,  39.80716458,  37.58362546,  32.39769704,\n",
       "        35.45048257,  31.23446481,  24.48478321,  33.28615723,\n",
       "        38.04368164,  37.15737267,  31.71297469,  25.26658017,\n",
       "        30.101515  ,  32.71897655,  28.42735376,  28.42999168,\n",
       "        27.2913215 ,  23.74446671,  24.11878941,  27.40241209,\n",
       "        16.32993575,  13.39695213,  20.01655581,  19.86205904,\n",
       "        21.28604604,  24.07796482,  24.20603792,  25.04201534,\n",
       "        24.91709097,  29.93762975,  23.97709054,  21.69931969,\n",
       "        37.51051381,  43.29459357,  36.48121427,  34.99129701,\n",
       "        34.80865729,  37.16296374,  40.9823638 ,  34.44211691,\n",
       "        35.83178068,  28.24913647,  31.22022312,  40.83256202,\n",
       "        39.31768808,  25.71099424,  22.30344878,  27.20551341,\n",
       "        28.51386352,  35.47494122,  36.11110647,  33.80004807,\n",
       "        35.61141951,  34.84311742,  30.35359323,  35.31260262,\n",
       "        38.79684808,  34.33296541,  40.34038636,  44.67339923,\n",
       "        31.5955473 ,  27.35994642,  20.09520596,  27.04518524,\n",
       "        27.21674397,  26.91105226,  33.43602979,  34.40228785,\n",
       "        31.83374181,  25.82416035,  24.43687139,  28.46348891,\n",
       "        27.36916176,  19.54441878,  29.11480679,  31.90852699,\n",
       "        30.77325183,  28.9430835 ,  28.88108106,  32.79876794,\n",
       "        33.20356949,  30.76568546,  35.55843485,  32.70725436,\n",
       "        28.64759861,  23.59388439,  18.5461558 ,  26.88429024,\n",
       "        23.28485442,  25.55002201,  25.48337323,  20.54343769,\n",
       "        17.61406384,  18.37627933,  24.29187594,  21.3257202 ,\n",
       "        24.88826131,  24.87143049,  22.87255605,  19.4540234 ,\n",
       "        25.11948741,  24.66816374,  23.68209656,  19.33951725,\n",
       "        21.17636041,  24.25306588,  21.59311197,  19.98766667,\n",
       "        23.34079584,  22.13973959,  21.55349196,  20.61808868,\n",
       "        20.1607571 ,  19.28455466,  22.16593919,  21.24893735,\n",
       "        21.42985456,  30.32874523,  22.04915396,  27.70610125,\n",
       "        28.54595004,  16.54657063,  14.78278261,  25.27336772,\n",
       "        27.54088054,  22.14633467,  20.46081206,  20.54472332,\n",
       "        16.88194391,  25.40066956,  14.32299547,  16.5927403 ,\n",
       "        19.63224597,  22.7117302 ,  22.19946949,  19.1989151 ,\n",
       "        22.66091019,  18.92059374,  18.22715359,  20.22444386,\n",
       "        37.47946099,  14.29172583,  15.53697148,  10.82825817,\n",
       "        23.81134987,  32.64787163,  34.61163401,  24.94604102,\n",
       "        26.00259724,   6.12085728,   0.78021126,  25.311373  ,\n",
       "        17.73465914,  20.22593282,  15.83834861,  16.83742401,\n",
       "        14.43123608,  18.47647773,  13.42427933,  13.05677824,\n",
       "         3.27646485,   8.05936467,   6.13903114,   5.62271213,\n",
       "         6.44935154,  14.20597451,  17.21022671,  17.29035065,\n",
       "         9.89064351,  20.21972222,  17.94511052,  20.30017588,\n",
       "        19.28790318,  16.33300008,   6.56843662,  10.87541577,\n",
       "        11.88704097,  17.81098929,  18.25461066,  12.99282707,\n",
       "         7.39319053,   8.25609561,   8.07899971,  19.98563715,\n",
       "        13.69651744,  19.83511412,  15.2345378 ,  16.93112419,\n",
       "         1.69347406,  11.81116263,  -4.28300934,   9.55007844,\n",
       "        13.32635521,   6.88351077,   6.16827417,  14.56933235,\n",
       "        19.59292932,  18.1151686 ,  18.52011987,  13.13707457,\n",
       "        14.59662601,   9.8923749 ,  16.31998048,  14.06750301,\n",
       "        14.22573568,  13.00752251,  18.13277547,  18.66645496,\n",
       "        21.50283795,  17.00039379,  15.93926602,  13.32952716,\n",
       "        14.48949211,   8.78366731,   4.8300317 ,  13.06115528,\n",
       "        12.71101472,  17.2887624 ,  18.73424906,  18.05271013,\n",
       "        11.49855612,  13.00841512,  17.66975577,  18.12342294,\n",
       "        17.51503231,  17.21307203,  16.48238543,  19.40079737,\n",
       "        18.57392951,  22.47833186,  15.24179836,  15.78327609,\n",
       "        12.64853778,  12.84121049,  17.17173661,  18.50906858,\n",
       "        19.02803874,  20.16441773,  19.76975335,  22.42614937,\n",
       "        20.31750314,  17.87618837,  14.3391341 ,  16.93715603,\n",
       "        16.98716629,  18.59431701,  20.16395155,  22.97743546,\n",
       "        22.45110639,  25.5707207 ,  16.39091112,  16.09765427,\n",
       "        20.52835689,  11.5429045 ,  19.20387482,  21.86820603,\n",
       "        23.47052203,  27.10034494,  28.57064813,  21.0839881 ,\n",
       "        19.4490529 ,  22.2189221 ,  19.65423066,  21.324671  ,\n",
       "        11.86231364,   8.22260592,   3.65825168,  13.76275951,\n",
       "        15.93780944,  20.62730097,  20.61035443,  16.88048035,\n",
       "        14.01017244,  19.10825534,  21.29720741,  18.45524217,\n",
       "        20.46764235,  23.53261729,  22.37869798,  27.62934247,\n",
       "        26.12983844,  22.34870269])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# train the model on the entire dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# predict the response values for the observations in X (\"test the model\")\n",
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the predicted response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To evaluate a model, we also need an **evaluation metric:**\n",
    "\n",
    "- Numeric calculation used to **quantify** the performance of a model\n",
    "- Appropriate metric depends on the **goals** of your problem\n",
    "\n",
    "Most common choices for regression problems:\n",
    "\n",
    "- **R-squared**: Percentage of variation explain by the model (\"reward function\" since higher is better)\n",
    "- **Mean Squared Error**: Average squared distance between the prediction and the correct answer (\"loss function\" since lower is better)\n",
    "\n",
    "In this case, we'll use mean squared error since it is more interpretable in a prediction context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute mean squared error using a function from metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.8977792177\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is known as **training mean squared error** because we are evaluating the model on the same data we used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problems-with-training-and-testing-on-the-same-data\"></a>\n",
    "### Problems with training and testing on the same data\n",
    "\n",
    "- Goal is to estimate likely performance of a model on **out-of-sample data**\n",
    "- But, maximizing training mean squared error rewards **overly complex models** that won't necessarily generalize\n",
    "- Unnecessarily complex models **overfit** the training data:\n",
    "    - Will do well when tested using the in-sample data\n",
    "    - May do poorly on out-of-sample data\n",
    "    - Learns the \"noise\" in the data rather than the \"signal\"\n",
    "    - From Quora: [What is an intuitive explanation of overfitting?](http://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)\n",
    "\n",
    "**Thus, training MSE is not a good estimate of out-of-sample MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-procedure--traintest-split\"></a>\n",
    "### Evaluation procedure #2: Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Split the dataset into two pieces: a **training set** and a **testing set**.\n",
    "2. Train the model on the **training set**.\n",
    "3. Test the model on the **testing set**, and evaluate how well we did.\n",
    "\n",
    "What does this accomplish?\n",
    "\n",
    "- Model can be trained and tested on **different data** (we treat testing data like out-of-sample data).\n",
    "- Response values are known for the testing set, and thus **predictions can be evaluated**.\n",
    "\n",
    "This is known as **testing Mean Squared Error** because we are evaluating the model on an independent \"test set\" that was not used during model training.\n",
    "\n",
    "**Testing MSE is a better estimate of out-of-sample performance than training MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we dive into train_test_split, lets understanding \"unpacking\" syntax.\n",
    "\n",
    "Unpacking in itself allows us to break down the contents of an object and assign it equally to several variables sumultaneously.\n",
    "\n",
    "Lets create a packed object (boxed) than then unpack it using a `for loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipment: package_1 | Shipment Contents: directions_1\n",
      "Shipment: package_2 | Shipment Contents: directions_2\n",
      "Shipment: package_3 | Shipment Contents: directions_3\n",
      "Shipment: package_4 | Shipment Contents: directions_4\n"
     ]
    }
   ],
   "source": [
    "# lets start with two lists who are related in some manner.\n",
    "package = ['package_1','package_2','package_3','package_4']\n",
    "directions = ['directions_1','directions_2','directions_3','directions_4']\n",
    "# we'll zip them together to form the associate combos\n",
    "boxed = zip(package, directions)\n",
    "\n",
    "\n",
    "# we can then use the `for Obj-1, Obj-2 in` to isolate the values we need\n",
    "for p, d in boxed:\n",
    "    print 'Shipment: %s | Shipment Contents: %s' % (p,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using a `for loop` to unpack an output we can just assign the results assuming we know exaclty how many results need to be assigned.  We can thing of `boxed` as being composed of 4 sub-compnents, we can use a for loop to help us break the subcomponents out OR use the unpacking method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box1, box2, box3, box4 = boxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('package_1', 'directions_1')\n",
      "('package_3', 'directions_3')\n"
     ]
    }
   ],
   "source": [
    "print box1\n",
    "print box3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of train-test-split, we do an unpackage assignment to the return value of a function, which the below code exemplifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function that takes a argument to act up \n",
    "def min_max(nums):\n",
    "    smallest = min(nums)\n",
    "    largest = max(nums)\n",
    "    # the function returns a list in the below order\n",
    "    return [smallest, largest, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "# we can assign the returned list to a single variable,\n",
    "min_and_max = min_max([1, 2, 3])\n",
    "print(min_and_max)\n",
    "print(type(min_and_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# OR, because we know the list is composed of 3 elements, \n",
    "# assign each element to its own variable.\n",
    "the_min, the_max, five = min_max([1, 2, 3])\n",
    "\n",
    "print the_max\n",
    "print the_min\n",
    "print five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `train_test_split` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(379, 13)\n",
      "(127, 13)\n"
     ]
    }
   ],
   "source": [
    "# before splitting\n",
    "print(X.shape)\n",
    "\n",
    "# after splitting\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L,)\n",
      "(379L,)\n",
      "(127L,)\n"
     ]
    }
   ],
   "source": [
    "# before splitting\n",
    "print(y.shape)\n",
    "\n",
    "# after splitting\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_test_split](./assets/images/train_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `random_state` parameter\n",
    "\n",
    "The `random_state` is a pseudo-random number allowing us to reproduce our results every time we run them, but making it impossible to predict what are exact results will be if we chose a new random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "18  0.80271  0.0   8.14   0.0  0.538  5.456  36.6  3.7965  4.0  307.0   \n",
      "\n",
      "    PTRATIO       B  LSTAT  \n",
      "18     21.0  288.99  11.69  \n"
     ]
    }
   ],
   "source": [
    "# WITHOUT a random_state parameter\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# print the first element of each object\n",
    "print(X_train.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM   ZN  INDUS  CHAS    NOX    RM   AGE     DIS  RAD    TAX  \\\n",
      "502  0.04527  0.0  11.93   0.0  0.573  6.12  76.7  2.2875  1.0  273.0   \n",
      "\n",
      "     PTRATIO      B  LSTAT  \n",
      "502     21.0  396.9   9.08  \n",
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "307  0.04932  33.0   2.18   0.0  0.472  6.849  70.3  3.1827  7.0  222.0   \n",
      "\n",
      "     PTRATIO      B  LSTAT  \n",
      "307     18.4  396.9   7.53  \n",
      "502    20.6\n",
      "Name: MEDV, dtype: float64\n",
      "307    28.2\n",
      "Name: MEDV, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# WITH a random_state parameter\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# print the first element of each object\n",
    "print(X_train.head(1))\n",
    "print(X_test.head(1))\n",
    "print(y_train.head(1))\n",
    "print(y_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce Patsy\n",
    "\n",
    "We will make one more modification. Patsy is a library that allows you to quickly do simple transformations of data in a style similar to R.\n",
    "\n",
    "Rather than manually creating X and y, we will use the dmatricies function from Patsy to create the matricies and allow us to explore the effect of changing features on training and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: split X and y into training and testing sets (using random_state for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices(\"MEDV ~ AGE + RM\", data=boston, return_type=\"dataframe\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: train the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: test the model on the testing set, and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.9690557877\n",
      "42.0017522149\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print(metrics.mean_squared_error(y_train, lr.predict(X_train)))\n",
    "print(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias-variance tradeoff](./assets/images/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to step 1 and try adding new variables and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error** decreases as model complexity increases (lower value of K)\n",
    "- **Testing error** is minimized at the optimum model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-test-performance-with-a-null-baseline\"></a>\n",
    "### Comparing test performance with a null baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When interpreting how predictive a model it's best to always compare it to a baseline using a dummy model, sometimes called a ZeroR model. A dummy model is simply using the mean, median, or most common value as prediction. This forms a benchmark to compare your model against and becomes especially important in classification where your null accuracy might be 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the baseline mean squared error using a null model\n",
    "How does this compare to what we achieved with linear regression. Is our model making an actual improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.7227974456\n"
     ]
    }
   ],
   "source": [
    "# Use .apply() to broadcast a mean for every prediction\n",
    "print(metrics.mean_squared_error(y_test, y_test.apply(np.mean, broadcast=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-folds-cross-validation\"></a>\n",
    "## K-folds cross-validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train test split gives as a nice inuition and tool, but it's a shame that we are tossing out a large chunk of our data for testing purposes.\n",
    "\n",
    "**How can we use the maximum amount of our data points while still ensuring model** integrity?\n",
    "\n",
    "1. Split our data into a number of different pieces (folds)\n",
    "2. Train using k-1 folds for training and a different fold for testing\n",
    "3. Average our model against EACH of those iterations\n",
    "4. Choose our model and TEST it against the final fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"leave-one-out-cross-validation\"></a>\n",
    "### Leave-one-out-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A special case of k-fold cross-validation is leave-one-out-cross-validation. Rather than taking 5-10 folds we take a fold of size n - 1 and leave one observation to test. \n",
    "\n",
    "Typically, 5-10 fold cross-validaiton is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-cross-validation-with-the-boston-data\"></a>\n",
    "### Intro to cross validation with the Boston data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross valiation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print \"~~~~ CROSS VALIDATION each fold ~~~~\"\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    lr = LinearRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lr.predict(X.iloc[test_index])))\n",
    "    scores.append(lr.score(X, y))\n",
    "    n+=1\n",
    "    print 'Model', n\n",
    "    print 'MSE:', mse_values[n-1]\n",
    "    print 'R2:', scores[n-1]\n",
    "\n",
    "\n",
    "print \"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\"\n",
    "print 'Mean of MSE for all folds:', np.mean(mse_values)\n",
    "print 'Mean of R2 for all folds:', np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "print np.mean(-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "print np.mean(cross_val_score(lr, X, y, cv=kf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cross validated approach here generated more overall error, which of the two approaches would predict new data more accurately: the single model or the cross validated, averaged one? Why?\n",
    "\n",
    "\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three-way-data-split\"></a>\n",
    "<a id=\"three-way-data-split\"></a>\n",
    "<a id=\"three-way-data-split\"></a>\n",
    "## Three way data split\n",
    "---\n",
    "\n",
    "The most common workflow is actually a combination of train-test-split and cross-validation. We take a train-test-split on our data right away and try not spend a lot of time using the test dataset. Instead we take our training data and tune our models using cross-validation. When we think we are done, we do one last test on the test data to to make sure we haven't accidently overfit to our training data.\n",
    "\n",
    "Even with good evaluation procedures it is incredible easy to overfit our models by including features that will not be available during production or leak information about our testing data in other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- If model selection and true error estimates are to be computed simultaneously,\n",
    "three disjoint data sets are best.\n",
    "- Training set: a set of example used for learning – what parameters of the\n",
    "classifier\n",
    "- Validation set: a set of examples used to tune the parameters of the classifier\n",
    "- Testing set: a set of examples used ONLY to assess the performance of the\n",
    "fully-trained classifier\n",
    "- Validation and testing must be separate data sets. Once you have the final\n",
    "model set, you cannot do any additional tuning after testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Divide data into training, validation, testing sets\n",
    "2. Select architecture (model type) and training parameters (k)\n",
    "3. Train the model using the training set\n",
    "4. Evaluate the model using the training set\n",
    "5. Repeat 2-4 selecting different architectures (models) and tuning parameters\n",
    "6. Select the best model\n",
    "7. Assess the model with the final testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"additional-resources\"></a>\n",
    "<a id=\"additional-resources\"></a>\n",
    "### Additional Resources\n",
    "- http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "- https://courses.cs.washington.edu/courses/cse546/12wi/slides/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
